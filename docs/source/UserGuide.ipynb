{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2586033b-4a2f-411a-b5e6-05159822626f",
   "metadata": {},
   "source": [
    "# User Guide"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c17affdf-979d-4059-8dc9-62dee242d0eb",
   "metadata": {},
   "source": [
    "## Quickstart\n",
    "\n",
    "This is a summary of a complete typical workflow.\n",
    "\n",
    " 1. Define the original dataset with DatasetWrapper.\n",
    "\n",
    "```python\n",
    "from iq_tool_box.datasets import DSWrapper\n",
    "\n",
    "ds_wrapper = DSWrapper(data_path=data_path)\n",
    "```\n",
    "\n",
    " 2. Define the modifications intended for each experiment. In this case JPG\n",
    "Modifiers with quality from 10 to 90.\n",
    "\n",
    "```python\n",
    "from iq_tool_box.datasets import DSModifier_jpg\n",
    "\n",
    "ds_modifiers_list = [DSModifier_jpg(params={'quality': i}) for i in [10,30,50,70,90] ]\n",
    "```\n",
    "\n",
    " 3. Define the model execution method. In this case the training method is a python script, so we define PythonScriptExecutionTask. Additionally hyperparameter variations can be added. In this case the epochs and learning rate is varied. The tool will then loop through all possible combinations of the variations. The user can also set the number of repetitions.\n",
    "\n",
    "\n",
    "```python\n",
    "from iq_tool_box.experiments import ExperimentSetup\n",
    "\n",
    "experiment = ExperimentSetup(\n",
    "   experiment_name=\"MyFirstExperiment\",\n",
    "   task_instance=task,\n",
    "   ref_dsw_train=ds_wrapper,\n",
    "   ds_modifiers_list=ds_modifiers_list,\n",
    "   repetitions=5,\n",
    "   extra_train_params={\n",
    "      'epochs':[10,15,20],\n",
    "      'lr':[1e-5,1e-6,1e-7]\n",
    "   }\n",
    ")\n",
    "\n",
    "experiment.execute()\n",
    "\n",
    "```\n",
    "\n",
    " 4. The information from the executed experiment can be collected in a json. Also a dataframe suitable for visualization tools (see next step) can be extracted from\n",
    "\n",
    "```python\n",
    "from iq_tool_box.experiments import ExperimentInfo\n",
    "\n",
    "experiment_info = ExperimentInfo(experiment_name)\n",
    "\n",
    "runs = experiment_info.runs\n",
    "\n",
    "df = experiment_info.get_df(\n",
    "   ds_params=[\"modifier\"],\n",
    "   metrics=['rmse','epochs','lr'],\n",
    "   dropna = True,\n",
    "   fields_to_float_lst = ['rmse','lr'],\n",
    "   fields_to_int_lst = ['epochs']\n",
    ")\n",
    "```\n",
    "\n",
    " 5. Visualizations can be made from the tool. In this case plots of root mean square error against learning rate variations are showed. This is one plot for each epoch (legend) in a shared chart.\n",
    "\n",
    "\n",
    "```python\n",
    "from iq_tool_box.experiments import ExperimentVisual\n",
    "\n",
    "ev = ExperimentVisual(\n",
    "   df,\n",
    "   os.path.join(data_path, \"mod-rmse-lr-epoch.png\")\n",
    ")\n",
    "\n",
    "ev.visualize(\n",
    "   xvar=\"lr\",\n",
    "   yvar=\"rmse\",\n",
    "   legend_var=\"epochs\",\n",
    "   title=\"rmse - lr\"\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28767d98-48e6-4eaa-8de9-05196bca78ce",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Conventions\n",
    "\n",
    "In **iquaflow** conventions are prefered over configurations. \n",
    "\n",
    "### Dataset Formats\n",
    "\n",
    "*  IQToolBox understands a dataset as a folder containing a sub-folder\n",
    "   with images and ground truth in json format. Datasets that does not\n",
    "   follow this format should be changed in order to perform experiments.\n",
    "\n",
    "*  In case of detection or segmentation tasks, the preferred formats\n",
    "   are:\n",
    "\n",
    "   *  Json in COCO format.\n",
    "   *  GeoJson with the minimum required fields (\"image_filename\", \"class_id\", \"geometry\").\n",
    "   *  A folder named maskes with images corresponding to the segmentation annotations.\n",
    "\n",
    "*  IQToolbox primarily works with COCO json ground truth adopted by most\n",
    "   of the datasets and models of the field. In case that the dataset is\n",
    "   in other format, the user can transform it to COCO\n",
    "   https://blog.roboflow.ai/how-to-convert-annotations-from-voc-xml-to-coco-json/\n",
    "   Otherwise, IQToolBox can not perform sanity neither statistics checks\n",
    "\n",
    "*  For other kind of tasks, such as image generation, it is only\n",
    "   necessary to have the ground truth in a json format. Alternatively,\n",
    "   IQT can recognize a dataset without any ground truth file\n",
    "\n",
    "*  When the dataset is modified, iquaflow creates a modified copy of the\n",
    "   dataset in its parent folder. As a convention, iquaflow adds to the name\n",
    "   of the original dataset a “#” followed by the name of the\n",
    "   modification as you can see in the following image.\n",
    " \n",
    "### Output Formats\n",
    "\n",
    "The packaged model could write in the output temporary folder the following files in order to be parsed as experiment parameters and metrics:\n",
    "\n",
    "* **results.json**: Json with keys as the name of parameter, values as a number related to the metric or an array reference to a sequence of values of that parameter.\n",
    "\n",
    "{\n",
    " \"train_f1\": 0.83,\n",
    " \"val_f1\": 0.78,\n",
    " \"test_f1\": 0.79,\n",
    " \"train_focal_loss\": [1.34, 1.29, 1.24, …., 0.01]\n",
    " \"val_focal_loss\": [1.34, 1.29, 1.24, …., 0.01]\n",
    "}\n",
    "\n",
    "* **output.json** : Output of the model (this allows to avoid reproducing experiments in the future in case it is wanted to test a new metric for former experiments) in a folder named output. The format of this json file depends on the task of the DL model.\n",
    "\n",
    "* Bounding Box Detection: **output.json** consists of a COCO format json, containing as many elements as detections have been made in the dataset. Each of these elements looks as shown below.\n",
    "\n",
    "```python\n",
    "{\n",
    "    \"image_id\" : 85\n",
    "    \"iscrowd\" : 0\n",
    "    \"bbox\":[\n",
    "        522.5372924804688\n",
    "        474.1499938964844\n",
    "        28.968505859375\n",
    "        27.19696044921875\n",
    "    ]\n",
    "    \"area\": 2427.050960971974\n",
    "    \"category_id\": 1\n",
    "    \"id\": 1\n",
    "    score : 0.9709288477897644\n",
    "}\n",
    "```\n",
    "\n",
    "  * Image generation: The json may contain the relative path to the generated images. Imagine the packaged model is Super Resolution model that generates five super resolution images. The package may store a folder named `generated_sr_image` in the output temporary file with this five images. Hence the **output.json** should be as following:\n",
    "\n",
    "```python\n",
    "{\n",
    " [\n",
    "   \"generated_sr_image/image_1.png\",\n",
    "   \"generated_sr_image/image_2.png\",\n",
    "   \"generated_sr_image/image_3.png\",\n",
    "   \"generated_sr_image/image_4.png\",\n",
    "   \"generated_sr_image/image_5.png\",\n",
    " ]\n",
    "}\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd565b82-a028-479b-b325-b7c0fad4b18c",
   "metadata": {},
   "source": [
    "## Sanity check and statistics\n",
    "\n",
    "**SanityCheck** and **DSStatistics** are the classes that will perform sanity check and statistics of image datasets and ground truth. They are stand alone classes, it is to say they can work by proving the path folder of images and ground truth, or they can work with **DSWrapper** class.\n",
    "\n",
    "### Sanity check\n",
    "\n",
    "The SanityCheck module performs sanity to image datasets and ground truth. It can either work as standalone class or with DSWrapper class. It will remove all corrupted samples following the logic in the argument flags. The new sanitized dataset is located in output_path attribute from the SanityCheck instance. \n",
    "A usage example:\n",
    "\n",
    "```python\n",
    "from iq_tool_box.sanity import SanityCheck\n",
    "\n",
    "sc = SanityCheck(data_path, output_folder)\n",
    "sc.check_annotations()\n",
    "```\n",
    "\n",
    "Some relevant taskes performed are:\n",
    "\n",
    " * Finding duplicates in coco json images list\n",
    " * Check if the image format is a valid image file format.\n",
    " * Check integrity of one coco annotation.\n",
    " * Fix height and width in coco json images list\n",
    " * In geojson annotations, remove all rows containing a Nan value, empty geometries in any of the required field columns.\n",
    " * In geojson annotations, try to fix geometries with buffer = 0 and remove the persistent invalid geometries.\n",
    "\n",
    "Note the difference between missing, empty and invalid geometries in a geojson:\n",
    "\n",
    " * _Missing geometries:_ This is when the attribute geometry is empty or unknown. Most libraries load it as `None` type in python. These values were typically propagated in operations (for example in calculations of the area or of the intersection), or ignored in reductions such as unary_union.\n",
    " * _Empty geometries:_ This happens when the coordinates are empty despite having a geometry type defined. This can happen as a result of an intersection between two polygons that have no overlap.\n",
    " * _Invalid geometry:_ Problematic features such as edges of a polygon intersecting themselves. This could have happened due to a mistake from the annotator. For the case of invalid geometry. The tool will also attempt to fix them with buffer=0 functionality prior to removing. In future releases an additional argument to simplify geometries will be offered.\n",
    "\n",
    "\n",
    "\n",
    "### Statistics and exploration\n",
    "\n",
    "There are several statistics that can be calculated from the datasets, they can be estimated and summariezed in visualizations. The resulting calculated parameters can be exported as json and the plots as images. The default location is in a subfolder *stats* within the dataset. The module *DsStats* performs stats to image datasets and annotations. It can either work as standalone class or with DSWrapper class.\n",
    "A usage example:\n",
    "\n",
    "```python\n",
    "from iq_tool_box.ds_stats import DsStats\n",
    "\n",
    "dss = DsStats(data_path, output_folder)\n",
    "stats = dss.perform_stats(show_plots = True)\n",
    "```\n",
    "\n",
    "Statistics performed are:\n",
    "\n",
    " * Average height and width images\n",
    " * Class tags histogram\n",
    " * Image and bounding box aspect ratio and area histograms\n",
    " * Calculates the best fitting bounding box and rotated bounding box\n",
    " * High, width angle from bounding box and rotated bounding box\n",
    " * Compactness, centroid and area of the polygon\n",
    " * min, mean and max from a dataframe field\n",
    "\n",
    "There are also two interactive exploratory tools. One to visualzie the annotations an another for the images. These are:\n",
    "\n",
    " * notebook_annots_summary\n",
    " * notebook_imgs_preview\n",
    "\n",
    "Usage example:\n",
    "\n",
    "```python\n",
    "from iq_tool_box.ds_stats import DsStats\n",
    "\n",
    "DsStats.notebook_annots_summary(\n",
    "    df,\n",
    "    export_html_filename=html_filename,\n",
    "    fields_to_include=[\"image_filename\", \"class_id\", \"area\"],\n",
    "    show_inline=True,\n",
    ")\n",
    "```\n",
    "\n",
    "```python\n",
    "from iq_tool_box.ds_stats import DsStats\n",
    "\n",
    "DsStats.notebook_imgs_preview(\n",
    "        data_path=data_path,\n",
    "        sample=100,\n",
    "        size=100,\n",
    ")\n",
    "```\n",
    "\n",
    "They can be used in line in notebooks or export them in html interactively.\n",
    "\n",
    "[See a notebook with Statistics examples](https://publicgitlab.satellogic.com/iqf/iq_tool_box-/blob/master/notebooks/Statistics.ipynb)\n",
    "\n",
    "## Dataset\n",
    "**DSWrapper** is the class that IQF uses for identifying datasets. Currently, **DSWrapper** only parses datasets with the structure explained in [Dataset Formats](https://publicgitlab.satellogic.com/iqf/iq_tool_box-/wikis/Home/1.-General-Description#dataset-formats). Basically the dateset is defined by a folder that contains only a unique sub-folder with the images and json that describes the annotations. It is preferred that the ground truth json is in COCO format or geojson so it can be used with the rest of the tools.   \n",
    "\n",
    "Having the dataset conformed as mentioned before it is simply as providing the location path to the **DSWrapper**\n",
    "\n",
    "```python\n",
    "from iq_tool_box.datasets import DSWrapper\n",
    "ds_wrapper = DSWrapper(data_path=\"[path_to_the_dataset]\")\n",
    "```\n",
    "Internally IQT parses the structure helping the experiment tools to understand how the dateset is conformed.\n",
    "\n",
    "Afterwards the user can find parsed the principal datasets paths:\n",
    "\n",
    "```python\n",
    "ds_wrapper.parent_folder # It is Path of the folder containing the dataset\n",
    "ds_wrapper.data_path #Root path of the dataset\n",
    "ds_wrapper.data_input #Path of the folder that contains the images\n",
    "ds_wrapper.json_annotations #Path to the jsn annotations. Preferred COCO annotations\n",
    "ds_wrapper.geojson_annotations #Path to the geojson annotations.\n",
    "```\n",
    "Furthermore, **DSWrapper** contains an editable dictionary that describes the dataset. Initially this dictionary contains the key ds_name that is the name of the dataset. The user can populate this dictionary with any key/value parameter. Afterwards, this dictionary will be populated and changed automatically by **DSModifier** classes and it will be used for experiments logins.\n",
    "\n",
    "```python\n",
    "ds_wrapper.params #Contains metainfomation of the dataset. Initially {\"ds_name\":\"[name_of_the_dataset]\"}\n",
    "```\n",
    "\n",
    "## Modifiers\n",
    "\n",
    "Modifiers take a dataset D and process to obtain a D' dataset with some image/data processing (degradation, compression, enhancement...)\n",
    "\n",
    "### Using an existing modifier to run an experiment:\n",
    "Just import the desired modifier and run it\n",
    "```\n",
    "from iq_tool_box.datasets import DSModifier_jpg\n",
    "\n",
    "img_path = \"test_datasets/ds_coco_dataset/images\")\n",
    "jpg85 = DSModifier_jpg(params={\"quality\": 85})\n",
    "jpg85.modify(data_input=img_path)\n",
    "```\n",
    "\n",
    "After running, a `test_datasets/ds_coco_dataset#jpg85_modifier/images/` folder should be created with the modified images.\n",
    "\n",
    "### Adding a new modifier tool:\n",
    "In [modifier_jpg.py](https://publicgitlab.satellogic.com/emilio.tylson/iq_tool_box-/blob/master/iq_tool_box/datasets/modifier_jpg.py) you have a good guide on how to implement a new modifier, inheriting from DSModifier_dir and writing the internal `_mod_img()` member function.\n",
    "\n",
    "For further documentation see [link a sphynx doc for modifiers](https:ladocu.com)\n",
    "\n",
    "### Modifier reference\n",
    "\n",
    "A directory of implemented modifiers of different types can be found in the [Data Modifiers Reference](DataModifiersReference)\n",
    "\n",
    "## Experiment formulation\n",
    "IQT allows to formulate experiments taking as reference the modified training datase . In order to perform this task,\n",
    "the package provides tools that allows to automatize this kind of experiments that is composed by: \n",
    "\n",
    "* A reference dataset.\n",
    "* A list of dataset modifiers.\n",
    "* A encapsuled machine learning model.\n",
    "\n",
    "The first two components are covered by DSWrapper and DSModifer respectively. Following, we will explain how to use \n",
    "a encapsuled model.\n",
    "\n",
    "## TaskExecution\n",
    "\n",
    "In [Convention over Configuration](https://publicgitlab.satellogic.com/iqf/iq_tool_box-/wikis/Home/1.-General-Description#convention-over-configuration)\n",
    "we explained how we expect the models to be packeg, and what are the mandatory and optional arguments expected the package.\n",
    "Under this assumptions IQT can to automatize experiments while the user has a flexible way of loging experiments information\n",
    "without knowing any specific login tool, he needs only to create a json file with the parameters that want to be tracked by IQT. Alternatively he can track \n",
    "any kind of file generated by the experiment by just saving the file in a temporary path (provided to the packaged model by IQT) \n",
    "or he even can store the raw results in a json for future computations.\n",
    "**TaskExecution** is the generic class that provides the mandatory and optional arguments to the packaged model when this is launched and it \n",
    "is also responable for translating all the experiment information to the mlflow tracking server. Hence, the user does not need\n",
    "to understand **MLFlow**, IQT internally uses **MLFlow** to organize the experiments.\n",
    "\n",
    "### PythonScriptTaskExecution\n",
    "\n",
    "This particular class extends from **TaskExecution** and knows how to execute a model that is encapsulated in a python script.\n",
    "In order to use it just instantiate the class with the path to the python script.\n",
    "\n",
    "```python\n",
    "task = PythonScriptTaskExecution(model_script_path=\"./path_to_script.py\")\n",
    "```\n",
    "\n",
    "Alternatively the user can execute the task, but is not recommendable since IQT will perform executions \n",
    "internally when the whole experiment is defined. In order to execute the run, the user must provide the experiment name,\n",
    "the name of the run  and the training dateset path or training **DSWrapper**. Optionally, the user can provide a training dataset path or ds_wrapper\n",
    "and a python dictionary with model hyper-parameters (that will be used when executing the package) \n",
    "\n",
    "```python\n",
    "task.train_val(\n",
    "            experiment_name=\"name of the experiment\",\n",
    "            run_name=\"test_run\",\n",
    "            train_ds=ds_wrapper_train,\n",
    "            val_ds=ds_wrapper_validation,\n",
    "            mlargs={\"lr\": 1e-6},\n",
    "        )\n",
    "```\n",
    "\n",
    "## ExperimentSetup\n",
    "\n",
    "Having defined all the components the user is able to perform a IQT experiment by using **ExperimentSetup**.\n",
    "The user must define the name of the experiment, the reference datasets, the list of datasets modifiers \n",
    "and the packaged model, as following\n",
    "\n",
    "```python\n",
    "experiment = ExperimentSetup(\n",
    "   experiment_name=\"experimentA\",\n",
    "   task_instance=PythonScriptTaskExecution(model_script_path=\"./path_to_script.py\"),\n",
    "   ref_dsw_train=DSWrapper(data_path=\"path_to_dataset\"),\n",
    "   ds_modifiers_list=[ DSModifier_jpg(params={'quality': i}) for i in [10,30,50,70,90] ]\n",
    ")\n",
    "\n",
    "```\n",
    "\n",
    "And then just execute the training by \n",
    "\n",
    "```python\n",
    "experiment.execute()\n",
    "```\n",
    "\n",
    "## Experiment Info\n",
    "\n",
    "This objects allows the user to manage the experiment information. It simplifies the access to MLFlow and allows to apply new metrics to previous executed experiments. Basic usage example:\n",
    "\n",
    "```python\n",
    "from iq_tool_box.experiments import ExperimentInfo\n",
    "\n",
    "experiment_info = ExperimentInfo(experiment_name)\n",
    "runs = experiment_info.get_mlflow_run_info() # runs is a python dict\n",
    "```\n",
    "\n",
    "These are the main methods:\n",
    "\n",
    " * get_mlflow_run_info > It gathers the experiment information ina a python dictionary.\n",
    " * apply_metric_per_run > Applies a new metric to previously executed experiments.\n",
    " * get_df > Retrives a selection of data in a suitable format so that it can be used as an input in the Visualization module.\n",
    "\n",
    "In the section Metrics and Visualization (just below) there are examples on how to use the last two methods.\n",
    "\n",
    "## Metrics and visualization\n",
    "\n",
    "Metrics and visualization tools will allow to:\n",
    "* Compute domain specific metrics from predictions made by the different models under tes.\n",
    "* Draw and visualize plots that sintetize the relevant information that might be obtained from the experiment.\n",
    "* Display several samples of the predictions made by the models, in order to illustrate the results, when appropriate.\n",
    "\n",
    "The module metrics contains functionalities to estimate metrics in your experiments. \n",
    "*BBDetectionMetrics* is an available metric that can be applied between bounding boxes of ground truth and predicted elements. They must be in COCO-format ( See [COCO detection](https://cocodataset.org/#detection-eval) and [COCO data](https://cocodataset.org/#format-data) ). When this metric is applied the metrics from COCOeval (See [COCO detection](https://cocodataset.org/#detection-eval) ) are estimated.\n",
    "\n",
    " - Custom metrics can be created by inheriting the class *Metrics*:\n",
    "\n",
    "```python\n",
    "from iq_tool_box.metrics import Metric\n",
    "\n",
    "class CustomMetric(Metric):\n",
    "    def __init__(self) -> None:\n",
    "        self.metric_names = coco_eval_metrics_names\n",
    "    def apply(self, predictions: str, gt_path: str) -> Any:\n",
    "        # Your custom code here\n",
    "        # Then return a dictionary of names and values for each metric\n",
    "        return {k: v for k, v in zip(metric_names, stats)}\n",
    "```\n",
    "\n",
    " - To calculate a metric to an executed experiment do:\n",
    "\n",
    "```python\n",
    "from iq_tool_box.experiments import ExperimentInfo\n",
    "\n",
    "experiment_info = ExperimentInfo(experiment_name)\n",
    "my_custom_metric = CustomMetric()\n",
    "experiment_info.apply_metric_per_run( my_custom_metric, json_annotations_name )\n",
    "```\n",
    "\n",
    "Apart from the visualization tools explained in the Sanity check and Statistics section, there are also tools for plotting the results. On one hand there is the [mlflow](https://mlflow.org/) service which is launched by `mlflow ui --host 0.0.0.0` and then accessed in the browser `http://ip_address_of_your_mlflow_server:5000` The Tracking UI lets you visualize, search and compare runs, as well as download run artifacts or metadata for analysis in other tools. If you log runs to a local mlruns directory, run mlflow ui in the directory above it, and it loads the corresponding runs. The UI contains the following key features:\n",
    " * Experiment-based run listing and comparison\n",
    " * Searching for runs by parameter or metric value\n",
    " * Visualizing run metrics\n",
    " * Downloading run results\n",
    "\n",
    "On the other hand there is the *ExperimentVisual* class. It offers both inline and saved files plotting utilities. It is designed so that it retrieves a dataframe extracted from an *ExperimentInfo* and then used as an input. See some examples in [Visual Notebooks](https://publicgitlab.satellogic.com/iqf/iq_tool_box-/blob/master/notebooks/Visualizations.ipynb). See also some code examples in the Typical workflow section (just below)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fec56524-aa32-4094-8735-829ea0f0cf02",
   "metadata": {},
   "source": [
    "## Development\n",
    "\n",
    "### Package Overview\n",
    "\n",
    "The python package structure of this tool box is based on [cookiecutter](http://gitlab.local/juanlu/cookiecutter-satellogic-ground). This library provides a standard workflow for developing production level packages. The tools that\n",
    "will be used are:\n",
    "1. [setuptools](https://pypi.org/project/setuptools/) for packaging\n",
    "1. [versioneer](https://pypi.org/project/versioneer/) for versioning\n",
    "1. [GitLab CI](https://about.gitlab.com/stages-devops-lifecycle/continuous-integration/) for continuous integration\n",
    "1. [tox](https://pypi.org/project/tox/3.2.0/) for managing test environments\n",
    "1. [pytest](https://pypi.org/project/pytest/) for tests\n",
    "1. [sphinx](https://pypi.org/project/Sphinx/) for documentation\n",
    "1. [black](https://pypi.org/project/black/), [flake8](https://pypi.org/project/flake8/2.0/) and [isort](https://pypi.org/project/isort/) for style checks\n",
    "1. [mypy](https://pypi.org/project/mypy/) for type checks\n",
    "\n",
    "More information can be found in:\n",
    "1. https://packaging.python.org/tutorials/packaging-projects/\n",
    "1. https://python-packaging.readthedocs.io/en/latest/minimal.html\n",
    "1. https://www.learnpython.org/en/Modules_and_Packages\n",
    "\n",
    "### Environment installation\n",
    "\n",
    "This repository does not require any specific python environment. In our case we use Python 3.7. Hence, we do recommend create a new environment with Python 3.7 and pip. The file ​*setup.py​* allows to install ​*iq_tool_box​* as a python package via pip. Once you have created your new environment, you only need to clone locally the repository:\n",
    "\n",
    "```\n",
    "git clone https://publicgitlab.satellogic.com/iqf/iq_tool_box-.git\n",
    "\n",
    "```\n",
    "and then do the wallowing command to install the *iq_tool_box* as a softlink in\n",
    "the environment:\n",
    "```\n",
    "python -m pip install -e .\n",
    "```\n",
    "Dependencies are defined in ​*setup.cfg* ​under ​install_requires​ tag. So first install the package\n",
    "in your local environment and then add the dependency in the ​*setup.cfg* ​with its corresponding\n",
    "version.\n",
    "\n",
    "\n",
    "### Documentation\n",
    "\n",
    "We use Sphinx to automatically update our documentation. This allows to maintain package documentation\n",
    "updated at the same time new code is added (as long the code is commented). The documentation and Sphinx configuration can be found inside ​/doc​. \n",
    "\n",
    "Under the ​/doc ​folder type in console\n",
    "```\n",
    "make html\n",
    "```\n",
    "Sphinx will generate under ​/doc/build/html ​the desired html documentation.\n",
    "You can also use tox:\n",
    "```\n",
    "tox -e docs\n",
    "```\n",
    "\n",
    "More information ​about Sphinx can be found in [here](https://www.writethedocs.org/guide/tools/sphinx/).\n",
    "\n",
    "### Continuous integration\n",
    "\n",
    "In our project we use TOX. This tool allows to manage multiple environments in order to automatically validate code. More information about TOX can be found in [here](https://github.com/tox-dev/tox).\n",
    "\n",
    "For quality check you only need to run:\n",
    "```\n",
    "tox -e check\n",
    "```\n",
    "For automatic code reformat:\n",
    "```\n",
    "tox -e reformat\n",
    "```\n",
    "For executing all test for first time use\n",
    "```\n",
    "tox -r -e py36 && tox -r -e py37 \n",
    "```\n",
    "Alternatively, if it is not the first time it is not necesary to recreate the tox envirement\n",
    "\n",
    "```\n",
    "tox -e py36 && tox -e py37 \n",
    "```\n",
    "\n",
    "Note: CI terminology for python can be found in [here](https://www.patricksoftwareblog.com/setting-up-gitlab-ci-for-a-python-application/)\n",
    "\n",
    "### Test\n",
    "\n",
    "Unit tests are performed using [PyTest](https://docs.pytest.org/en/stable/usage.html​). All tests are included in ​test the folder located in the repository main folder.\n",
    "Once you have created a new test module, *e.g.* test_new_module, that includes python assertions, simply type in the console *pytest* or: \n",
    "```\n",
    "pytest <module name>\n",
    "```\n",
    "to run the tests. \n",
    "\n",
    "We strongly recommned to use “test_” as the prefix of every test you create. \n",
    "\n",
    "You can also run test manually using *tox*(recommended) (use *-r* parametar for creating tox environment for the first time): ​\n",
    "```\n",
    "tox -e py36 && tox -e py37\n",
    "```\n",
    "More information ​can be found in https://docs.python-guide.org/writing/tests/\n",
    "\n",
    "\n",
    "### Initial development process\n",
    "\n",
    "Below we describe usual steps when developing from scratch:\n",
    "\n",
    "1. Setup python environment:\n",
    "  \n",
    "   ```\n",
    "   conda create -n iqt-env python=3.7\n",
    "   ```\n",
    "\n",
    "1. Clone repository:\n",
    "\n",
    "   ```\n",
    "   git clone https://publicgitlab.satellogic.com/emilio.tylson/iq_tool_box-.git\n",
    "   ```\n",
    "\n",
    "1. Create branch:\n",
    "\n",
    "   ```\n",
    "   git checkout -b <new_branch_name>\n",
    "   ```\n",
    "\n",
    "1. Install soft link via:\n",
    "\n",
    "   ```\n",
    "   python -m pip install -e . \n",
    "   ```\n",
    "\n",
    "1. Create test that defines modules functionality.\n",
    "1. Solve the test by adding package functionality.\n",
    "1. If new branch pulled use `tox -r` to recreate tox environments.\n",
    "1. Reformat code:\n",
    "   ```\n",
    "   python -m pip install tox \n",
    "   tox -e reformat\n",
    "   ```\n",
    "1. Check code and solve:\n",
    "   \n",
    "   ```\n",
    "   tox -e check\n",
    "   ```\n",
    "\n",
    "1. Run tests:\n",
    "\n",
    "   ```\n",
    "   tox -e py36 && tox -e py37\n",
    "   ```\n",
    "\n",
    "1. Push to remote branch.\n",
    "1. Create MR and assign reviewer.\n",
    "1. Refreshing local repository for running tests (after `pip install -e .`):\n",
    "   \n",
    "   ```\n",
    "   tox -r -e py37Sphinx\n",
    "   ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db5849e1-3fd2-4948-8dc9-a423dc044adc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py37",
   "language": "python",
   "name": "py37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
